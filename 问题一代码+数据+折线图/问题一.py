# -----------------------------------------------------------
#   predict_qingming_rain_2025_validation.py
# -----------------------------------------------------------
#   åŠŸèƒ½ï¼šåˆ¤å®šæ¸…æ˜å‡æœŸ(4â€‘4~4â€‘6)æ˜¯å¦"é›¨çº·çº·"ï¼Œä½¿ç”¨é€»è¾‘å›å½’æ¨¡å‹
#   â€‘ å°†2025å¹´æ•°æ®åˆ†ç¦»ç”¨äºæ¨¡å‹éªŒè¯
#   â€‘ åŸºäº2025å¹´ä¹‹å‰çš„æ•°æ®è®­ç»ƒæ¨¡å‹
#   â€‘ é¢„æµ‹2025å¹´æ¦‚ç‡å¹¶ä¸å®é™…ç»“æœå¯¹æ¯”
#   â€‘ é¢„æµ‹2026å¹´æ¦‚ç‡
# -----------------------------------------------------------
#   è¿è¡Œç¤ºä¾‹:
#       python predict_qingming_rain_2025_validation.py
# -----------------------------------------------------------

from pathlib import Path
from datetime import datetime, date
import sys
import re
import pandas as pd
import numpy as np

# Import necessary components from scikit-learn
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler


# ========= [1]  è·¯å¾„ä¸å­—æ®µé…ç½® ===============================================
# !! æ³¨æ„: è¯·å°†æ­¤è·¯å¾„ä¿®æ”¹ä¸ºæ‚¨å®é™…å­˜å‚¨æ•°æ®æ–‡ä»¶çš„ç›®å½• !!
DATA_DIR   = Path("D:\\å­¦ä¹ èµ„æ–™\\æ•°æ¨¡\\2025æ•°ç»´æ¯\\2025æ•°ç»´æ¯Cé¢˜è§£é¢˜æ€è·¯+å®Œæ•´ä»£ç ï¼\\2025æ•°ç»´æ¯Cé¢˜è§£é¢˜æ€è·¯+å®Œæ•´ä»£ç ï¼\\S6 2025æ•°ç»´æ¯Cé¢˜1-4é—®å¯è¿è¡Œä»£ç +å‚è€ƒç­”æ¡ˆï¼\\è¥¿å®‰å¤©æ°”")
FILE_REGEX = re.compile(r".*\.(xlsx|csv)$", re.I)

# æ ¹æ®æ‚¨çš„è¡¨å¤´å›¾ç‰‡é…ç½®åˆ—å
DATE_COL   = "æ—¶é—´"
PRECIP_COL = "é™æ°´é‡(mm)"
TEMP_COL = 'æ°”æ¸©(â„ƒ)'
HUMIDITY_COL = 'ç›¸å¯¹æ¹¿åº¦(%)'
PRES_COL = 'æ°”å‹(hPa)'
DEW_PT_COL = 'éœ²ç‚¹æ¸©åº¦(â„ƒ)'
WIND_SPD_COL = 'é£é€Ÿ(m/s)'
AVG_WIND_SPD_3H_COL = None
MIN_TEMP_COL_DAILY = 'æœ€ä½æ¸©åº¦(â„ƒ)'
MAX_TEMP_COL_DAILY = 'æœ€é«˜æ¸©åº¦(â„ƒ)'
WIND_DIR_INST_COL = None
MAX_WIND_SPD_COL_INST = None
PRECIP_PHEN_COL = None

# ========= [2]  "é›¨çº·çº·"åˆ¤å®šè§„åˆ™ =============================================
RAIN_MM_RANGE = (1.0, 25.0)      # é›¨çº·çº·é™é›¨é‡èŒƒå›´ (mm)
MIN_RAIN_PERIODS_24H = 3         # æœ€å°é™é›¨æ—¶æ®µæ•°ï¼ˆ24å°æ—¶å†…ï¼‰

# ========= [3]  è¯»å–æ•°æ® ======================================================
def read_one_file(fp: Path) -> pd.DataFrame:
    """è¯»å–å•ä¸ª Excel/CSV æ–‡ä»¶åˆ° DataFrame"""
    print(f"\nè¯»å–æ–‡ä»¶: {fp.name}")
    try:
        if fp.suffix.lower() in (".xls", ".xlsx"):
            df = pd.read_excel(fp, engine="openpyxl")
        else:
            try:
                df = pd.read_csv(fp, encoding="utf-8")
            except UnicodeDecodeError:
                try:
                    df = pd.read_csv(fp, encoding="gbk")
                except Exception as e:
                    print(f"âŒ æ— æ³•ä½¿ç”¨ utf-8 æˆ– gbk ç¼–ç è¯»å– CSV æ–‡ä»¶ {fp.name}: {e}")
                    return pd.DataFrame()
    except FileNotFoundError:
        print(f"âŒ æ–‡ä»¶æœªæ‰¾åˆ°: {fp.name}")
        return pd.DataFrame()
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶ {fp.name} æ—¶å‡ºé”™: {e}")
        return pd.DataFrame()
    return df

def load_dataset(root: Path) -> pd.DataFrame:
    """åŠ è½½æŒ‡å®šç›®å½•/æ–‡ä»¶çš„å…¨éƒ¨æ•°æ®å¹¶åˆå¹¶"""
    dfs = []
    if root.is_file():
        dfs.append(read_one_file(root))
    elif root.is_dir():
        files = [f for f in root.iterdir() if FILE_REGEX.match(f.name)]
        if not files:
            sys.exit(f"âš ï¸ åœ¨ç›®å½• {root} ä¸­æœªæ‰¾åˆ°ä»»ä½• .xlsx / .csv æ•°æ®æ–‡ä»¶")
        dfs = [read_one_file(f) for f in files]
    else:
        sys.exit(f"âš ï¸ æŒ‡å®šçš„è·¯å¾„ {root} ä¸æ˜¯æœ‰æ•ˆçš„æ–‡ä»¶æˆ–ç›®å½•")

    dfs = [df for df in dfs if not df.empty]
    if not dfs:
        sys.exit("âš ï¸ æœªæˆåŠŸè¯»å–ä»»ä½•æ•°æ®æ–‡ä»¶ã€‚")

    df_all = pd.concat(dfs, ignore_index=True)

    global DATE_COL
    date_column_candidates = ["æ—¶é—´", "æ—¥æœŸ", "æ—¥/æ—¶", "æ—¥/æœˆ", "time", "date"]
    if DATE_COL not in df_all.columns:
        print(f"è­¦å‘Š: æ‰‹åŠ¨è®¾ç½®çš„æ—¥æœŸåˆ—å {DATE_COL!r} ä¸åœ¨è¡¨æ ¼åˆ—ä¸­ï¼Œå°è¯•è‡ªåŠ¨åŒ¹é…...")
        for col in date_column_candidates:
            if col in df_all.columns:
                DATE_COL = col
                print(f"æ‰¾åˆ°æ—¥æœŸåˆ—: {DATE_COL!r}")
                break
        else:
            print(f"âŒ æ‰¾ä¸åˆ°åˆé€‚çš„æ—¥æœŸåˆ—")

    return df_all

# Helper function to calculate yearly aggregated features for a specific period
def calculate_yearly_feature_for_period(df_input, years_to_include, col_name, month_filter, day_range_filter, agg_func, feature_name):
    """Calculates a yearly aggregated feature for a specified month/day range from an input DataFrame."""
    # Filter the input DataFrame by year first
    df_filtered_years = df_input[df_input['Year'].isin(years_to_include)].copy()

    if col_name not in df_filtered_years.columns:
        print(f"è­¦å‘Š: æ•°æ®ä¸­ç¼ºå°‘åˆ— {col_name!r}ï¼Œè·³è¿‡ç‰¹å¾ {feature_name} çš„è®¡ç®—ã€‚")
        return pd.Series(index=years_to_include, name=feature_name)

    # Filter data for the specified month and day range within the selected years
    if day_range_filter:
        df_period = df_filtered_years[(df_filtered_years['Month'] == month_filter) & (df_filtered_years['Day'].between(day_range_filter[0], day_range_filter[1]))].copy()
    else: # Filter by month only
        df_period = df_filtered_years[df_filtered_years['Month'] == month_filter].copy()

    if df_period.empty:
        # If no data for the period for any of the included years
        print(f"è­¦å‘Š: æŒ‡å®šå¹´ä»½ ({sorted(years_to_include)}) çš„ {month_filter}æœˆ{''.join(f'-{d}' for d in day_range_filter) if day_range_filter else ''} æ²¡æœ‰æ•°æ®ï¼Œæ— æ³•è®¡ç®—ç‰¹å¾ {feature_name}")
        return pd.Series(index=years_to_include, name=feature_name) # Return Series with NaNs for all years


    # Ensure column is numeric, coercing errors to NaN
    df_period[col_name] = pd.to_numeric(df_period[col_name], errors='coerce')

    # Group by year and apply aggregation function
    yearly_agg = df_period.groupby('Year')[col_name].agg(agg_func)
    yearly_agg.name = feature_name

    # Reindex to ensure all years in years_to_include are present (with NaN if no data for that year/period)
    yearly_agg = yearly_agg.reindex(years_to_include)

    return yearly_agg

# Helper function for count-based features (rainy days, rain periods)
def calculate_yearly_count_feature(df_input, years_to_include, col_name, month_filter, day_range_filter, count_type):
    """Calculates yearly count features (rainy days or rain periods)."""
    df_filtered_years = df_input[df_input['Year'].isin(years_to_include)].copy()

    if col_name not in df_filtered_years.columns:
        print(f"è­¦å‘Š: æ•°æ®ä¸­ç¼ºå°‘åˆ— {col_name!r}ï¼Œè·³è¿‡ç‰¹å¾ {count_type} çš„è®¡ç®—ã€‚")
        return pd.Series(index=years_to_include, name=f'March_{count_type}' if month_filter==3 else f'EarlyApril_{count_type}')

    # Filter data for the specified month and day range
    if day_range_filter:
        df_period = df_filtered_years[(df_filtered_years['Month'] == month_filter) & (df_filtered_years['Day'].between(day_range_filter[0], day_range_filter[1]))].copy()
    else: # Filter by month only
        df_period = df_filtered_years[df_filtered_years['Month'] == month_filter].copy()

    if df_period.empty:
        print(f"è­¦å‘Š: æŒ‡å®šå¹´ä»½ ({sorted(years_to_include)}) çš„ {month_filter}æœˆ{''.join(f'-{d}' for d in day_range_filter) if day_range_filter else ''} æ²¡æœ‰æ•°æ®ï¼Œæ— æ³•è®¡ç®—ç‰¹å¾ {count_type}")
        series_name = f'March_{count_type}' if month_filter==3 else f'EarlyApril_{count_type}'
        return pd.Series(index=years_to_include, name=series_name, data=0) # Assume 0 count if no data? Or NaN? Let's return 0 count if no data for the period

    try:
        df_period[col_name] = pd.to_numeric(df_period[col_name], errors='coerce').fillna(0) # Fill precip NaNs with 0

        if count_type == 'Rainy_Days':
            # Group by year and day, sum precip per day
            daily_precip = df_period.groupby(['Year', 'Month', 'Day'])[col_name].sum()
            # Count days where sum is > 0, grouped by year
            count_series = daily_precip[daily_precip > 0].groupby('Year').size()
        elif count_type == 'Rain_Period_Count':
            # Filter for records with precipitation > 0
            df_period_rain_periods = df_period[df_period[col_name] > 0].copy()
            # Group by year and count the number of records (periods)
            count_series = df_period_rain_periods.groupby('Year').size()
        else:
            raise ValueError("Invalid count_type. Use 'Rainy_Days' or 'Rain_Period_Count'.")

        series_name = f'March_{count_type}' if month_filter==3 else f'EarlyApril_{count_type}'
        count_series.name = series_name

        # Reindex to include all specified years, fill missing years (no counts) with 0
        count_series = count_series.reindex(years_to_include, fill_value=0)

        return count_series

    except Exception as e:
        print(f"  - è®¡ç®— {count_type} æ—¶å‡ºé”™: {e}")
        series_name = f'March_{count_type}' if month_filter==3 else f'EarlyApril_{count_type}'
        return pd.Series(index=years_to_include, name=series_name, data=np.nan) # Return NaNs on calculation error


# ========= [4]  ä¸»æµç¨‹ ========================================================
def main():
    print(f"å¼€å§‹åŠ è½½æ•°æ®é›†ï¼Œä»ç›®å½•/æ–‡ä»¶: {DATA_DIR}")
    df = load_dataset(DATA_DIR)

    if df.empty:
        sys.exit("âš ï¸ æœªåŠ è½½åˆ°ä»»ä½•æœ‰æ•ˆæ•°æ®ï¼Œç¨‹åºç»ˆæ­¢ã€‚")

    # Ensure Date column is datetime type and extract Year, Month, Day
    if DATE_COL not in df.columns or not pd.api.types.is_datetime64_any_dtype(df[DATE_COL]):
        print(f"é”™è¯¯: æ—¥æœŸåˆ— {DATE_COL!r} ä¸å¯ç”¨æˆ–ä¸æ˜¯æ—¥æœŸæ—¶é—´ç±»å‹ï¼Œå°è¯•é‡æ–°è½¬æ¢...")
        try:
            df[DATE_COL] = pd.to_datetime(df[DATE_COL], errors='coerce')
            df.dropna(subset=[DATE_COL], inplace=True)
            if df.empty:
                sys.exit("âŒ æ—¥æœŸåˆ—è½¬æ¢å¤±è´¥ä¸”æ²¡æœ‰æœ‰æ•ˆæ—¥æœŸï¼Œç¨‹åºç»ˆæ­¢ã€‚")
            print("æ—¥æœŸåˆ—è½¬æ¢æˆåŠŸã€‚")
        except Exception as e:
            sys.exit(f"âŒ æ—¥æœŸåˆ—æœ€ç»ˆæ— æ³•è½¬æ¢ä¸ºæ—¥æœŸæ—¶é—´ç±»å‹: {e}")

    df['Year'] = df[DATE_COL].dt.year
    df['Month'] = df[DATE_COL].dt.month
    df['Day'] = df[DATE_COL].dt.day

    # Ensure essential numeric columns exist and convert them
    numeric_cols_to_check = [PRECIP_COL, TEMP_COL, HUMIDITY_COL, PRES_COL, DEW_PT_COL, WIND_SPD_COL, AVG_WIND_SPD_3H_COL, MIN_TEMP_COL_DAILY, MAX_TEMP_COL_DAILY, MAX_WIND_SPD_COL_INST]
    for col in numeric_cols_to_check:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')
            if col == PRECIP_COL:
                df[col] = df[col].fillna(0) # Fill NaN precip with 0
        else:
            print(f"è­¦å‘Š: æ•°æ®ä¸­ç¼ºå°‘é¢„æœŸåˆ— {col!r}ã€‚")


    # View overall data years
    all_years = df['Year'].dropna().unique()
    if len(all_years) > 0:
        print(f"\næ•°æ®é›†åŒ…å«çš„å¹´ä»½: {sorted(all_years)}")
        print(f"æ€»å¹´ä»½æ•°: {len(all_years)}")
    else:
        print("\nè­¦å‘Š: æ•°æ®é›†ä¸­æ²¡æœ‰è¯†åˆ«åˆ°æœ‰æ•ˆçš„å¹´ä»½ä¿¡æ¯ã€‚")


    # Filter for Qingming holiday dates (Apr 4-6) across all available years
    df_qm_all_years = df[df[DATE_COL].dt.strftime("%m-%d").isin(["04-04", "04-05", "04-06"])].copy()


    # === Calculate the Target Variable (é›¨çº·çº· marker) for ALL historical years ===
    # This is needed to get the actual outcome for 2025
    frequent_rain_dates_all_qm = []

    if not df_qm_all_years.empty and PRECIP_COL in df_qm_all_years.columns:
        df_qm_all_valid_rain = df_qm_all_years[
            (~pd.isna(df_qm_all_years[PRECIP_COL])) &
            (df_qm_all_years[PRECIP_COL] > 0) &
            (df_qm_all_years[PRECIP_COL] >= RAIN_MM_RANGE[0]) &
            (df_qm_all_years[PRECIP_COL] <= RAIN_MM_RANGE[1])
            ].copy()

        if not df_qm_all_valid_rain.empty:
            rain_periods_per_day_all_qm = df_qm_all_valid_rain.groupby(df_qm_all_valid_rain[DATE_COL].dt.date).size()
            frequent_rain_dates_all_qm = rain_periods_per_day_all_qm[rain_periods_per_day_all_qm >= MIN_RAIN_PERIODS_24H].index.tolist()

            # Print frequent rain dates for all years for context
            print(f"\næ»¡è¶³é¢‘ç¹é™æ°´æ¡ä»¶çš„æ¸…æ˜æ—¥æœŸ (æ‰€æœ‰å¹´ä»½, â‰¥ {MIN_RAIN_PERIODS_24H} ä¸ªæ»¡è¶³é™é›¨é‡èŒƒå›´çš„æ—¶æ®µ):")
            if frequent_rain_dates_all_qm:
                for d in sorted(frequent_rain_dates_all_qm):
                    count = rain_periods_per_day_all_qm.get(d, 0)
                    print(f"- {d.strftime('%Y-%m-%d')} ({count} ä¸ªæ—¶æ®µ)")
            else:
                print("æ— ")
        else:
            print("\næ‰€æœ‰å¹´ä»½çš„æ¸…æ˜æœŸé—´éƒ½æ²¡æœ‰æ»¡è¶³é™é›¨é‡èŒƒå›´çš„è®°å½•ã€‚")


    # Determine if each historical year had "é›¨çº·çº·"
    yearly_flag_all = {}
    years_to_check_all = sorted(df['Year'].dropna().unique())
    for year in years_to_check_all:
        is_fenfen_this_year = False
        for day_offset in range(3): # Apr 4, 5, 6
            try:
                qm_date = date(year, 4, 4 + day_offset)
                if qm_date in frequent_rain_dates_all_qm:
                    is_fenfen_this_year = True
                    break
            except ValueError:
                continue
        yearly_flag_all[year] = is_fenfen_this_year

    yearly_flag_series_all = pd.Series(yearly_flag_all, name="qingming_has_rain_fenfen").sort_index()
    yearly_flag_series_all.index.name = "Year"

    # === Separate Data for Training (Before 2025) and Validation (2025) ===
    train_years = [year for year in yearly_flag_series_all.index if year < 2025]
    year_2025 = 2025
    years_for_prediction = [2026] # We will also predict 2026

    if year_2025 not in yearly_flag_series_all.index:
        sys.exit(f"âŒ æ•°æ®ä¸­ä¸åŒ…å«å¹´ä»½ {year_2025} çš„è®°å½•ï¼Œæ— æ³•è¿›è¡Œ 2025 å¹´çš„éªŒè¯ã€‚è¯·ç¡®ä¿æ•°æ®åŒ…å« {year_2025} å…¨å¹´çš„è®°å½•ã€‚")

    # Get the actual outcome for 2025
    y_2025_actual = yearly_flag_series_all.loc[year_2025]
    print(f"\n2025 å¹´æ¸…æ˜å‡æœŸå®é™…æ˜¯å¦ä¸ºã€é›¨çº·çº·ã€: {'æ˜¯' if y_2025_actual else 'å¦'}")


    # === Calculate Features (X) for Training and Validation ===
    print("\n============== è®¡ç®—æ¨¡å‹ç‰¹å¾ ==============")

    # --- Calculate Features for all years (including 2025) first ---
    # Use all years available in the data to calculate features
    years_for_feature_calc = sorted(df['Year'].dropna().unique())
    X_all = pd.DataFrame(index=years_for_feature_calc)
    X_all['Year_Feature'] = X_all.index

    print("\n--- è®¡ç®—3æœˆç‰¹å¾ (æ‰€æœ‰å¹´ä»½) ---")
    X_all = X_all.join(calculate_yearly_feature_for_period(df, years_for_feature_calc, TEMP_COL, 3, None, 'mean', 'March_Avg_Temp'))
    X_all = X_all.join(calculate_yearly_feature_for_period(df, years_for_feature_calc, HUMIDITY_COL, 3, None, 'mean', 'March_Avg_Humidity'))
    X_all = X_all.join(calculate_yearly_feature_for_period(df, years_for_feature_calc, PRES_COL, 3, None, 'mean', 'March_Avg_Pressure'))
    X_all = X_all.join(calculate_yearly_feature_for_period(df, years_for_feature_calc, PRECIP_COL, 3, None, 'sum', 'March_Total_Precip'))
    X_all = X_all.join(calculate_yearly_count_feature(df, years_for_feature_calc, PRECIP_COL, 3, None, 'Rainy_Days'))
    X_all = X_all.join(calculate_yearly_count_feature(df, years_for_feature_calc, PRECIP_COL, 3, None, 'Rain_Period_Count'))
    # Add more March features...

    print("\n--- è®¡ç®—4æœˆ1-3æ—¥ç‰¹å¾ (æ‰€æœ‰å¹´ä»½) ---")
    X_all = X_all.join(calculate_yearly_feature_for_period(df, years_for_feature_calc, TEMP_COL, 4, (1, 3), 'mean', 'EarlyApril_Avg_Temp'))
    X_all = X_all.join(calculate_yearly_feature_for_period(df, years_for_feature_calc, HUMIDITY_COL, 4, (1, 3), 'mean', 'EarlyApril_Avg_Humidity'))
    X_all = X_all.join(calculate_yearly_feature_for_period(df, years_for_feature_calc, PRECIP_COL, 4, (1, 3), 'sum', 'EarlyApril_Total_Precip'))
    X_all = X_all.join(calculate_yearly_count_feature(df, years_for_feature_calc, PRECIP_COL, 4, (1, 3), 'Rainy_Days'))
    X_all = X_all.join(calculate_yearly_count_feature(df, years_for_feature_calc, PRECIP_COL, 4, (1, 3), 'Rain_Period_Count'))
    # Add more Early April features...


    # --- Separate X and y into Training (pre-2025) and 2025 ---
    X_train_full = X_all.loc[train_years].copy()
    y_train_full = yearly_flag_series_all.loc[train_years].copy()

    # Get 2025 features
    X_2025 = X_all.loc[[year_2025]].copy()

    # Drop any rows (years) from training data where features are all NaN (e.g., no data for that year/period)
    # Or handle NaNs using imputation (recommended)
    initial_train_years = X_train_full.shape[0]
    # X_train_full = X_train_full.dropna(how='all') # Drop if ALL features are NaN for a year
    # y_train_full = y_train_full.loc[X_train_full.index] # Align y
    # if initial_train_years > X_train_full.shape[0]:
    #     print(f"\nè­¦å‘Š: è®­ç»ƒé›†ä¸­ {initial_train_years - X_train_full.shape[0]} å¹´å› ç‰¹å¾ç¼ºå¤±è¢«ä¸¢å¼ƒã€‚")


    # Check if 2025 features have NaNs (this is expected if data for March/EarlyApril 2025 is missing)
    if X_2025.isna().any().any():
        print(f"\nè­¦å‘Š: 2025 å¹´çš„ç‰¹å¾æ•°æ®åŒ…å«ç¼ºå¤±å€¼:\n{X_2025.isna().sum()[X_2025.isna().sum() > 0]}")
        print("è¿™äº›ç¼ºå¤±å€¼å°†åœ¨å¡«å……æ­¥éª¤ä¸­å¤„ç†ã€‚")


    # Ensure we have enough data for training
    if X_train_full.shape[0] > 5 and y_train_full.nunique() > 1:
        print(f"\nç”¨äºè®­ç»ƒæ¨¡å‹çš„å†å²å¹´ä»½æ•° (æˆªæ­¢åˆ° 2024): {X_train_full.shape[0]}")
        print(f"è®­ç»ƒé›†ç›®æ ‡å˜é‡åˆ†å¸ƒ:\n{y_train_full.value_counts()}")

        # === Train and Use Logistic Regression Model ===
        print("\n============== è®­ç»ƒé€»è¾‘å›å½’æ¨¡å‹ ==============")

        try:
            # 1. åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›† (ä»é¢„2025å¹´çš„æ•°æ®ä¸­åˆ†)
            # test_size here is for internal model validation, not the 2025 external validation
            X_train, X_test, y_train, y_test = train_test_split(X_train_full, y_train_full, test_size=0.3, random_state=42, stratify=y_train_full)
            print(f"å†…éƒ¨è®­ç»ƒé›†å¹´ä»½æ•°: {X_train.shape[0]}, å†…éƒ¨æµ‹è¯•é›†å¹´ä»½æ•°: {X_test.shape[0]}")

            # 2. åº”ç”¨ Imputation (å¡«å……ç¼ºå¤±å€¼) å’Œ Scaling (æ ‡å‡†åŒ–)
            # Fit imputer and scaler ONLY on the internal training data (X_train)
            print("æ‹Ÿåˆ Imputer å’Œ Scaler (åŸºäºå†…éƒ¨è®­ç»ƒé›†)...")
            imputer = SimpleImputer(strategy='mean') # Or 'median', etc.
            scaler = StandardScaler() # Or None if no scaling is desired

            X_train_processed = imputer.fit_transform(X_train)
            if scaler:
                X_train_processed = scaler.fit_transform(X_train_processed)
            X_train_processed_df = pd.DataFrame(X_train_processed, columns=X_train.columns, index=X_train.index) # Convert back to DF

            # Transform internal test set, 2025 data, and 2026 data using the *fitted* imputer and scaler
            print("è½¬æ¢å†…éƒ¨æµ‹è¯•é›†ã€2025å¹´æ•°æ®...")
            X_test_processed = imputer.transform(X_test)
            if scaler:
                X_test_processed = scaler.transform(X_test_processed)
            X_test_processed_df = pd.DataFrame(X_test_processed, columns=X_test.columns, index=X_test.index)

            X_2025_processed = imputer.transform(X_2025)
            if scaler:
                X_2025_processed = scaler.transform(X_2025_processed)
            X_2025_processed_df = pd.DataFrame(X_2025_processed, columns=X_2025.columns, index=X_2025.index)


            # 3. åˆ›å»ºå¹¶è®­ç»ƒé€»è¾‘å›å½’æ¨¡å‹ (ä½¿ç”¨å¤„ç†åçš„å†…éƒ¨è®­ç»ƒé›†)
            model = LogisticRegression(random_state=42, solver='liblinear')
            model.fit(X_train_processed_df, y_train)
            print("é€»è¾‘å›å½’æ¨¡å‹è®­ç»ƒå®Œæˆã€‚")

            # 4. è¯„ä¼°æ¨¡å‹ (åœ¨å†…éƒ¨æµ‹è¯•é›†ä¸Š)
            print("\næ¨¡å‹åœ¨å†…éƒ¨æµ‹è¯•é›†ä¸Šçš„è¯„ä¼°:")
            y_pred_test = model.predict(X_test_processed_df)
            y_pred_proba_test = model.predict_proba(X_test_processed_df)[:, 1]

            print(f"å†…éƒ¨æµ‹è¯•é›† Accuracy: {accuracy_score(y_test, y_pred_test):.4f}")
            if y_test.nunique() > 1:
                print(f"å†…éƒ¨æµ‹è¯•é›† Precision: {precision_score(y_test, y_pred_test):.4f}")
                print(f"å†…éƒ¨æµ‹è¯•é›† Recall: {recall_score(y_test, y_pred_test):.4f}")
                print(f"å†…éƒ¨æµ‹è¯•é›† F1 Score: {f1_score(y_test, y_pred_test):.4f}")
                print(f"å†…éƒ¨æµ‹è¯•é›† AUC-ROC: {roc_auc_score(y_test, y_pred_proba_test):.4f}")
            else:
                print("å†…éƒ¨æµ‹è¯•é›†åªåŒ…å«å•ä¸€ç±»åˆ«ï¼Œè·³è¿‡ Precision, Recall, F1, AUC è®¡ç®—ã€‚")


            # === é¢„æµ‹ 2025 å¹´å¹¶è¿›è¡ŒéªŒè¯ ===
            print("\n============== é¢„æµ‹ 2025 å¹´å¹¶è¿›è¡ŒéªŒè¯ ==============")
            prob_fenfen_2025_pred = model.predict_proba(X_2025_processed_df)[:, 1][0]
            print(f"2025 å¹´æ¸…æ˜å‡æœŸå®é™…æ˜¯å¦ä¸ºã€é›¨çº·çº·ã€: {'æ˜¯' if y_2025_actual else 'å¦'}")
            print(f"æ¨¡å‹é¢„æµ‹ 2025 å¹´æ¸…æ˜å‡æœŸå‡ºç°ã€é›¨çº·çº·ã€çš„æ¦‚ç‡ â‰ˆ {prob_fenfen_2025_pred:.1%}")

            # ä½ å¯ä»¥æ ¹æ®éœ€è¦æ·»åŠ éªŒè¯é€»è¾‘ï¼Œä¾‹å¦‚ï¼š
            # å¦‚æœ prob_fenfen_2025_pred > 0.5 (æˆ–ä½ çš„é˜ˆå€¼)ï¼Œé¢„æµ‹ä¸ºâ€œæ˜¯â€ï¼Œç„¶åä¸ y_2025_actual å¯¹æ¯”
            # Compare predicted class vs actual class
            # predicted_class_2025 = (prob_fenfen_2025_pred > 0.5).astype(int)
            # print(f"åŸºäºé˜ˆå€¼0.5çš„é¢„æµ‹ç»“æœ: {'æ˜¯' if predicted_class_2025 else 'å¦'} (å®é™…ç»“æœ: {'æ˜¯' if y_2025_actual else 'å¦'})")
            # print(f"é¢„æµ‹æ˜¯å¦æ­£ç¡®: {predicted_class_2025 == y_2025_actual}")


            # === é¢„æµ‹ 2026 å¹´ ===
            print("\n============== é¢„æµ‹ 2026 å¹´ ==============")
            # Prepare 2026 data - You NEED to replace NaN with your estimated values for 2026 features!
            # This DataFrame MUST have the EXACT SAME COLUMNS as X_train_full
            X_2026 = pd.DataFrame([[2026] + [np.nan] * (len(X_train_full.columns) - 1)], columns=X_train_full.columns)
            # IMPORTANT: Replace np.nan below with your estimated values for 2026 for each feature!
            # X_2026['March_Avg_Temp'] = estimated_2026_march_avg_temp
            # ... fill all columns

            # Transform 2026 data using the SAME fitted imputer and scaler
            if X_2026.isna().any().any():
                print("è­¦å‘Š: 2026 å¹´çš„ç‰¹å¾æ•°æ®åŒ…å«ç¼ºå¤±å€¼ã€‚å°†ä½¿ç”¨è®­ç»ƒé›†å‡å€¼å¡«å……ã€‚") # Imputer handles this, but you should provide real estimates
            X_2026_processed = imputer.transform(X_2026)
            if scaler:
                X_2026_processed = scaler.transform(X_2026_processed)
            X_2026_processed_df = pd.DataFrame(X_2026_processed, columns=X_2026.columns, index=X_2026.index)


            prob_fenfen_2026_pred = model.predict_proba(X_2026_processed_df)[:, 1][0]
            print(f"ã€é¢„æµ‹ã€‘2026 å¹´æ¸…æ˜å‡æœŸè¥¿å®‰å‡ºç°ã€é›¨çº·çº·ã€çš„æ¦‚ç‡ (åŸºäºé€»è¾‘å›å½’æ¨¡å‹) â‰ˆ {prob_fenfen_2026_pred:.1%}")


        except Exception as e:
            print(f"\nâŒ æ¨¡å‹è®­ç»ƒæˆ–é¢„æµ‹å‡ºé”™: {e}")
            print("è¯·æ£€æŸ¥æ•°æ®æ˜¯å¦åŒ…å«è¶³å¤Ÿå¹´ä»½ï¼Œä»¥åŠç‰¹å¾æå–å’Œå¤„ç†æ˜¯å¦æ­£ç¡®ã€‚")
            model = None

    else:
        print("\nç”¨äºè®­ç»ƒæ¨¡å‹çš„å†å²å¹´ä»½æ•°æ®ä¸è¶³ æˆ– ç›®æ ‡å˜é‡ç±»åˆ«æ•°ä¸è¶³ï¼Œæ— æ³•è®­ç»ƒé€»è¾‘å›å½’æ¨¡å‹ã€‚")
        model = None


    # === Save Historical Results ===
    # Save the historical rain fenfen flags (all years)
    out_hist_flag = DATA_DIR / "qingming_rain_fenfen_historical_flags_all_years.csv"
    try:
        out_hist_flag.parent.mkdir(parents=True, exist_ok=True)
        yearly_flag_series_all.to_csv(out_hist_flag, header=True, encoding="utf-8-sig")
        print(f"\nğŸ“„ æ‰€æœ‰å¹´ä»½çš„å†å²é›¨çº·çº·æ ‡è®°å·²ä¿å­˜: {out_hist_flag}")
    except Exception as e:
        print(f"\nâŒ ä¿å­˜å†å²æ ‡è®°å¤±è´¥: {e}")

    # Optional: Save the features and target variable used for modeling training (pre-2025)
    # if 'X_train_full' in locals() and not X_train_full.empty and 'y_train_full' in locals() and not y_train_full.empty:
    #     df_train_model_data = X_train_full.copy()
    #     df_train_model_data['qingming_has_rain_fenfen'] = y_train_full
    #     model_train_data_out = DATA_DIR / "qingming_model_train_data_pre2025.csv"
    #     df_train_model_data.to_csv(model_train_data_out, encoding="utf-8-sig")
    #     print(f"ğŸ“„ æ¨¡å‹è®­ç»ƒæ•°æ® (ç‰¹å¾ä¸æ ‡è®°, é¢„2025å¹´) å·²ä¿å­˜: {model_train_data_out}")


if __name__ == "__main__":
    main()